{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m156.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface_hub, tokenizers, transformers\n",
      "Successfully installed huggingface_hub-0.26.3 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers huggingface_hub; mkdir semeval25-unlearning-model; mkdir semeval25-unlearning-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-18.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m274.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-18.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167663d458a4405d9aef1c9cb5f9df8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d2de21d8944c10b3d5c2c266d35ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3b1b2299594e7c800b649ad5a0e8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "hf_token = \"\"\n",
    "\n",
    "## Fetch and load model:\n",
    "snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-finetuned-semeval25-unlearning', token=hf_token, local_dir='semeval25-unlearning-model')\n",
    "model = AutoModelForCausalLM.from_pretrained('semeval25-unlearning-model')\n",
    " \n",
    "## Fetch and load dataset:\n",
    "snapshot_download(repo_id='llmunlearningsemeval2025organization/semeval25-unlearning-dataset-public', token=hf_token, local_dir='semeval25-unlearning-data', repo_type=\"dataset\")\n",
    "retain_train_df = pd.read_parquet('semeval25-unlearning-data/data/retain_train-00000-of-00001.parquet', engine='pyarrow') # Retain split: train set\n",
    "retain_validation_df = pd.read_parquet('semeval25-unlearning-data/data/retain_validation-00000-of-00001.parquet', engine='pyarrow') # Retain split: validation set\n",
    "forget_train_df = pd.read_parquet('semeval25-unlearning-data/data/forget_train-00000-of-00001.parquet', engine='pyarrow') # Forget split: train set\n",
    "forget_validation_df = pd.read_parquet('semeval25-unlearning-data/data/forget_validation-00000-of-00001.parquet', engine='pyarrow') # Forget split: validation set\n",
    "!mkdir train validation\n",
    "retain_train_df.to_json('train/retain.jsonl'); forget_train_df.to_json('train/forget.jsonl')\n",
    "retain_validation_df.to_json('validation/retain.jsonl'); forget_validation_df.to_json('validation/forget.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (2.2.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089baacba85463f8993f373ad753779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 250 items.\n",
      "Loaded 250 items.\n",
      "Step 1/50 - Forget Loss: 0.3123, Retain Loss: 423.2497\n",
      "Step 2/50 - Forget Loss: 5.8398, Retain Loss: 120.5967\n",
      "Step 3/50 - Forget Loss: 7.2305, Retain Loss: 127.6733\n",
      "Step 4/50 - Forget Loss: 8.7266, Retain Loss: 116.8926\n",
      "Step 5/50 - Forget Loss: 11.6797, Retain Loss: 111.2015\n",
      "Step 6/50 - Forget Loss: 13.8906, Retain Loss: 107.0791\n",
      "Step 7/50 - Forget Loss: 15.9922, Retain Loss: 101.4759\n",
      "Step 8/50 - Forget Loss: 18.0625, Retain Loss: 95.0451\n",
      "Step 9/50 - Forget Loss: 20.1406, Retain Loss: 91.9494\n",
      "Step 10/50 - Forget Loss: 22.1875, Retain Loss: 115.1084\n",
      "Step 11/50 - Forget Loss: 24.3125, Retain Loss: 106.2331\n",
      "Step 12/50 - Forget Loss: 26.2656, Retain Loss: 97.6098\n",
      "Step 13/50 - Forget Loss: 28.4375, Retain Loss: 90.4844\n",
      "Step 14/50 - Forget Loss: 30.5938, Retain Loss: 83.4505\n",
      "Step 15/50 - Forget Loss: 32.8125, Retain Loss: 80.0005\n",
      "Step 16/50 - Forget Loss: 35.0000, Retain Loss: 78.1970\n",
      "Step 17/50 - Forget Loss: 37.2188, Retain Loss: 77.0833\n",
      "Step 18/50 - Forget Loss: 39.4688, Retain Loss: 76.9053\n",
      "Step 19/50 - Forget Loss: 41.7188, Retain Loss: 76.2341\n",
      "Step 20/50 - Forget Loss: 44.0000, Retain Loss: 75.0639\n",
      "Step 21/50 - Forget Loss: 46.2500, Retain Loss: 74.8439\n",
      "Step 22/50 - Forget Loss: 48.5000, Retain Loss: 74.7483\n",
      "Step 23/50 - Forget Loss: 50.7188, Retain Loss: 74.4037\n",
      "Step 24/50 - Forget Loss: 52.8750, Retain Loss: 74.4109\n",
      "Step 25/50 - Forget Loss: 55.0000, Retain Loss: 73.7298\n",
      "Step 26/50 - Forget Loss: 57.1875, Retain Loss: 74.1178\n",
      "Step 27/50 - Forget Loss: 59.3438, Retain Loss: 73.6002\n",
      "Step 28/50 - Forget Loss: 61.5312, Retain Loss: 73.4266\n",
      "Step 29/50 - Forget Loss: 63.6875, Retain Loss: 74.2829\n",
      "Step 30/50 - Forget Loss: 65.8750, Retain Loss: 73.7344\n",
      "Step 31/50 - Forget Loss: 68.1250, Retain Loss: 73.3370\n",
      "Step 32/50 - Forget Loss: 70.3750, Retain Loss: 73.1855\n",
      "Step 33/50 - Forget Loss: 72.6250, Retain Loss: 73.0003\n",
      "Step 34/50 - Forget Loss: 74.8750, Retain Loss: 73.6668\n",
      "Step 35/50 - Forget Loss: 77.1250, Retain Loss: 72.9541\n",
      "Step 36/50 - Forget Loss: 79.4375, Retain Loss: 73.2267\n",
      "Step 37/50 - Forget Loss: 81.6875, Retain Loss: 74.0595\n",
      "Step 38/50 - Forget Loss: 83.9375, Retain Loss: 74.6268\n",
      "Step 39/50 - Forget Loss: 86.1875, Retain Loss: 73.9759\n",
      "Step 40/50 - Forget Loss: 88.3750, Retain Loss: 73.3019\n",
      "Step 41/50 - Forget Loss: 90.6250, Retain Loss: 72.7616\n",
      "Step 42/50 - Forget Loss: 92.7500, Retain Loss: 72.4429\n",
      "Step 43/50 - Forget Loss: 94.8750, Retain Loss: 72.3951\n",
      "Step 44/50 - Forget Loss: 97.0000, Retain Loss: 71.9716\n",
      "Step 45/50 - Forget Loss: 99.2500, Retain Loss: 71.8925\n",
      "Step 46/50 - Forget Loss: 101.4375, Retain Loss: 71.4671\n",
      "Step 47/50 - Forget Loss: 103.6875, Retain Loss: 72.6881\n",
      "Step 48/50 - Forget Loss: 105.8750, Retain Loss: 72.7292\n",
      "Step 49/50 - Forget Loss: 108.0625, Retain Loss: 72.2541\n",
      "Step 50/50 - Forget Loss: 110.2500, Retain Loss: 75.1205\n",
      "Unlearned model saved to ./unlearning/output/unlearned_1b_model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "class JSONLDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_length=512):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load the data from the JSONL file\n",
    "        with open(jsonl_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                document = item.get(\"document\", \"\")\n",
    "                output = item.get(\"sentence_completion_task\", {}).get(\"output\", \"\")\n",
    "                self.data.append({\"input\": document, \"output\": output})\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} items.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(\n",
    "            item[\"input\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Tokenize the output (labels)\n",
    "        labels = self.tokenizer(\n",
    "            item[\"output\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "def gradient_ascent_unlearning(\n",
    "    model, tokenizer, retain_loader, forget_loader, output_path, lr=1e-4, num_steps=50, gradient_accumulation_steps=2, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        total_forget_loss = 0.0\n",
    "        total_retain_loss = 0.0\n",
    "\n",
    "        # Gradient ascent on the forget set\n",
    "        for i, batch in enumerate(forget_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Forward pass without labels for forget dataset\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Custom loss: maximize entropy or use negative log-likelihood\n",
    "                forget_loss = -torch.mean(logits)  # Example: reverse optimization\n",
    "\n",
    "            if forget_loss is not None:\n",
    "               scaler.scale(-forget_loss).backward()  # Maximize forget loss\n",
    "            else:\n",
    "               continue  # Skip invalid batches\n",
    "\n",
    "        if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(forget_loader):\n",
    "            if any(p.grad is not None for p in model.parameters()):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_forget_loss += forget_loss.item() if forget_loss is not None else 0\n",
    "        # Gradient descent on the retain set\n",
    "        for i, batch in enumerate(retain_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                retain_loss = outputs.loss\n",
    "\n",
    "            retain_loss = retain_loss / gradient_accumulation_steps\n",
    "            scaler.scale(retain_loss).backward()  # Scale the loss for mixed precision\n",
    "\n",
    "            # Gradient accumulation\n",
    "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(retain_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_retain_loss += retain_loss.item()\n",
    "\n",
    "        print(f\"Step {step + 1}/{num_steps} - Forget Loss: {total_forget_loss:.4f}, Retain Loss: {total_retain_loss:.4f}\")\n",
    "\n",
    "        # Clear cache to free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save the updated model\n",
    "    model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    print(f\"Unlearned model saved to {output_path}\")\n",
    "\n",
    "# Example Usage\n",
    "hf_token = \"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\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained('semeval25-unlearning-1B-model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMo-1B-0724-hf')\n",
    "\n",
    "# Define paths to the JSONL datasets\n",
    "retain_path = \"/teamspace/studios/this_studio/unlearning/semeval25-unlearning-data/mia_data/member.jsonl\"\n",
    "forget_path = \"/teamspace/studios/this_studio/unlearning/semeval25-unlearning-data/mia_data/nonmember.jsonl\"\n",
    "\n",
    "# Initialize datasets and DataLoaders\n",
    "retain_dataset = JSONLDataset(retain_path, tokenizer)\n",
    "forget_dataset = JSONLDataset(forget_path, tokenizer)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=1, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define output path for the updated model\n",
    "output_model_path = \"./unlearning/output/unlearned_1b_model\"\n",
    "\n",
    "# Perform gradient ascent unlearning\n",
    "gradient_ascent_unlearning(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    retain_loader=retain_loader,\n",
    "    forget_loader=forget_loader,\n",
    "    output_path=output_model_path,\n",
    "    lr=1e-4,\n",
    "    num_steps=50,\n",
    "    gradient_accumulation_steps=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
